{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1402a373",
   "metadata": {},
   "source": [
    "## Wisconsin Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcabc1b",
   "metadata": {},
   "source": [
    "## Machine Learning Classifiers \n",
    "\n",
    "The research paper I studied to provide analysis for this section of the project was called \"Diagnosis of Breast Cancer Pathology on the Wisconsin Dataset with the Help of Data Mining Classification and Clustering Techniques\". This research paper looks at five types of machine learning classifiers and how well they perform on the Wisconsin breast cancer dataset. Below I have given an overall review on the 5 different classifiers they used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a81fe",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier (J48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4e8df",
   "metadata": {},
   "source": [
    "The first classification algorithm we will look at that was applied to the Wisconsin breast cancer dataset is the ID3 algorithm which is also called J48. This practical algorithm divides features into groups by generating decision trees from datasets. It is easy and fast to use and the resulting output is comprehensible. This algorithm uses two concepts to create a decision tree, entropy and information gain. Entropy measures the impurity of a set, it assigns high uncertainty values to outcomes with low probability and low uncertainty values to outcomes with high probability. The information gain of an attribute is the reduction in entropy from partitioning the data according to an attribute. This helps partition the nodes of the tree by making the attribute with the highest information gain the root node and so on as you progress down the tree, the information gain of the attributes decreases, until there is no attributes left then we finish with the leaf nodes. Information gain has some drawbacks for example it tends to favour attributes that can take on a large number of different values. An alternative is the information gain ratio. There are some issues with decision tree learning such as a decision tree cannot consider relationships between two attributes and it doesn’t deal well with “noisy” data, missing attributes. The hypothesis found is sensitive to the training set. If you were to replace some training set data with new ones, the new one would be consistent with the original tree. Modifications to reduce this instability would be to alter the attribute selection procedure so that it is less sensitive to some of the training set data being replaced. [1][2][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae5110",
   "metadata": {},
   "source": [
    "###  Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4e883",
   "metadata": {},
   "source": [
    "The second classification algorithm that was applied to the dataset is Naïve Bayes algorithm. This algorithm is based around Bayes Theorem which calculates the probability of an event occurring based on prior knowledge that represents uncertainty before seeing any data. This theorem uses conditional probability which means events are mutually exclusive and can’t occur together i.e every attribute in the dataset is treated as independent of each other, knowing the Clump Thickness value of a tumor gives us no information on its Marginal Adhesion and vice versa. This assumption is why the algorithm is known as “naïve”. The other assumption with this algorithm is that every attribute is equal, all of them combined make an accurate output. There is no attribute with greater importance than the other attributes like we saw in the J48 algorithm. The process of this algorithm starts by analysing the data in a frequency table. Then you plug in the data to bayes theorem to get the posterior which is the probability distribution that represents uncertainty after seeing the data. The attribute with the highest posterior decides the outcome of prediction, this creates a classifier model. Naïve Bayes is much more faster than other algorithms and can be applied to smaller training sets. When its assumption of independence holds, it performs much better than other models. However in real life most attributes are dependent on each other so this would decrease the accuracy in its performance. An example of the use of this algorithm in the real world is in spam filtering where it identifies spam in your email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb530f49",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484ce55",
   "metadata": {},
   "source": [
    "The third algorithm mentioned in this research paper is multilayer perceptron. This algorithm has a structure of three main layers, the input layer, the hidden layer and the output layer. A dataset is processed through this algorithm by passing into the input layer first then the hidden layer and finally through the output layer. A Multilayer Perceptron(MLP) is a neural network and each neuron has inputs that are weighted just like the thetas in the linear regression equation. Each of these weights are given values, the larger the values the more complicated the network therefore it is favourable to keep the values small. The hidden layer and output layer are where most of the computations occur in this approximation function. MLP’s have an activation function that is used to map the weighted inputs to outputs of the network.[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475d360a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2284/2213808118.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from csv import reader\n",
    "\n",
    "file = open('breast-cancer-wisconsin.csv', 'w', )\n",
    "\n",
    "def load_dataset(file):\n",
    "    data = list()\n",
    "    with open(file, 'r') as f:\n",
    "        csv_reader = reader(f)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            data.append(row)\n",
    "    return np.array(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691472d",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d64d3",
   "metadata": {},
   "source": [
    "[1] https://www.hindawi.com/journals/abb/2022/6187275/\n",
    "[2] https://towardsdatascience.com/decision-trees-for-classification-id3-algorithm-explained-89df76e72df1\n",
    "[3] https://en.wikipedia.org/w/index.php?title=ID3_algorithm&oldid=970826747\n",
    "[4] https://machinelearningmastery.com/neural-networks-crash-course/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5473ce9",
   "metadata": {},
   "source": [
    "upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196bc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
